{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "eegIDZuDOkqf"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import math\n",
        "\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KZ0vV_kNi-wH"
      },
      "outputs": [],
      "source": [
        "# COCO class names\n",
        "\n",
        "class_names = [\"person\" , \"bicycle\" , \"car\" , \"motorcycle\" , \"airplane\" , \"bus\" , \"train\" , \"truck\" , \"boat\" , \"traffic light\" , \"fire hydrant\" , \"street sign\" , \"stop sign\" , \"parking meter\" , \"bench\" , \"bird\" , \"cat\" , \"dog\" , \"horse\" , \"sheep\" , \"cow\" , \"elephant\" , \"bear\" , \"zebra\" , \"giraffe\" , \"hat\" , \"backpack\" , \"umbrella\" , \"shoe\" , \"eye glasses\" , \"handbag\" , \"tie\" , \"suitcase\" ,\n",
        "\"frisbee\" , \"skis\" , \"snowboard\" , \"sports ball\" , \"kite\" , \"baseball bat\" ,\n",
        "\"baseball glove\" , \"skateboard\" , \"surfboard\" , \"tennis racket\" , \"bottle\" ,\n",
        "\"plate\" , \"wine glass\" , \"cup\" , \"fork\" , \"knife\" , \"spoon\" , \"bowl\" ,\n",
        "\"banana\" , \"apple\" , \"sandwich\" , \"orange\" , \"broccoli\" , \"carrot\" , \"hot dog\" ,\n",
        "\"pizza\" , \"donut\" , \"cake\" , \"chair\" , \"couch\" , \"potted plant\" , \"bed\" ,\n",
        "\"mirror\" , \"dining table\" , \"window\" , \"desk\" , \"toilet\" , \"door\" , \"tv\" ,\n",
        "\"laptop\" , \"mouse\" , \"remote\" , \"keyboard\" , \"cell phone\" , \"microwave\" ,\n",
        "\"oven\" , \"toaster\" , \"sink\" , \"refrigerator\" , \"blender\" , \"book\" ,\n",
        "\"clock\" , \"vase\" , \"scissors\" , \"teddy bear\" , \"hair drier\" , \"toothbrush\" , \"hair brush\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29nZdXmoPUWZ",
        "outputId": "89130df8-2ed7-41d7-d4a4-aff7009af039"
      },
      "outputs": [],
      "source": [
        "# Load faster rcnn\n",
        "faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODFcd5QeQGwj",
        "outputId": "fc697c61-a087-4004-de59-c48b0fb47c56"
      },
      "outputs": [],
      "source": [
        "# Load single shot detector\n",
        "ssd = torchvision.models.detection.ssd300_vgg16(pretrained = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSha7mOxQa3U",
        "outputId": "5bbc133c-9780-456b-8dee-cfa419983241"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# put to evaluation mode to not track gradients - just for inference\n",
        "faster_rcnn.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntJsdAakQda0",
        "outputId": "8b023299-262e-45e9-de3e-520ae4f51c93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SSD(\n",
              "  (backbone): SSDFeatureExtractorVGG(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): ReLU(inplace=True)\n",
              "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): ReLU(inplace=True)\n",
              "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (13): ReLU(inplace=True)\n",
              "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (15): ReLU(inplace=True)\n",
              "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (18): ReLU(inplace=True)\n",
              "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (20): ReLU(inplace=True)\n",
              "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (22): ReLU(inplace=True)\n",
              "    )\n",
              "    (extra): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "        (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (6): ReLU(inplace=True)\n",
              "        (7): Sequential(\n",
              "          (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "          (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
              "          (2): ReLU(inplace=True)\n",
              "          (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (4): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "        (3): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]], clip=True, scales=[0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05], steps=[8, 16, 32, 64, 100, 300])\n",
              "  (head): SSDHead(\n",
              "    (classification_head): SSDClassificationHead(\n",
              "      (module_list): ModuleList(\n",
              "        (0): Conv2d(512, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(1024, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(512, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 546, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): Conv2d(256, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (5): Conv2d(256, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (regression_head): SSDRegressionHead(\n",
              "      (module_list): ModuleList(\n",
              "        (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.48235, 0.45882, 0.40784], std=[0.00392156862745098, 0.00392156862745098, 0.00392156862745098])\n",
              "      Resize(min_size=(300,), max_size=300, mode='bilinear')\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# put to evaluation mode to not track gradients - just for inference\n",
        "ssd.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOCqRTt8ULrF",
        "outputId": "02a82c62-dcce-4809-f276-8134a71ba71c"
      },
      "outputs": [],
      "source": [
        "# load yolov8 large model\n",
        "yolo = YOLO('../Yolo-Weights/yolov8l.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "# transform to tensors\n",
        "transform = torchvision.transforms.ToTensor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C8bBANPVmSY"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87mJ-_6cVqcC"
      },
      "source": [
        "# **Detection on Video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "oYBFAfpoQe8V"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from time import time\n",
        "\n",
        "# Open the video file for processing\n",
        "cap = cv2.VideoCapture(\"video.mp4\")\n",
        "\n",
        "# Initialize a dictionary to store results\n",
        "results_fr = {\n",
        "    'inference_time': [],  # To store the time taken for inference\n",
        "    'conf_scores': [],     # To store confidence scores of detections\n",
        "    'class_names': [],     # To store class names for each detection\n",
        "    'bboxes': []           # To store bounding boxes of detections\n",
        "}\n",
        "\n",
        "while True:\n",
        "    # Read a frame from the video\n",
        "    success, img = cap.read()\n",
        "\n",
        "    # Break the loop if no frame is captured (end of video)\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    # Apply transformations to the frame (e.g., resizing, normalization)\n",
        "    transformed_img = transform(img)\n",
        "\n",
        "    # Start timing the inference process\n",
        "    start_time = time()\n",
        "    \n",
        "    # Perform object detection on the transformed image\n",
        "    pred = faster_rcnn([transformed_img])\n",
        "    \n",
        "    # End timing the inference process\n",
        "    end_time = time()\n",
        "\n",
        "    # Extract bounding boxes, labels, and scores from the prediction\n",
        "    bboxes, labels, scores = pred[0]['boxes'], pred[0]['labels'], pred[0]['scores']\n",
        "\n",
        "    # Filter out predictions with scores below the threshold (0.8 in this case)\n",
        "    num = torch.argwhere(scores > 0.8).shape[0]\n",
        "\n",
        "    # Initialize lists to store results for the current frame\n",
        "    classes = []\n",
        "    boxes = []\n",
        "    conf_scores = []\n",
        "\n",
        "    for i in range(num):\n",
        "        # Convert bounding box coordinates to integers\n",
        "        x1, y1, x2, y2 = bboxes[i].detach().numpy().astype('int')\n",
        "        boxes.append([x1, y1, x2, y2])\n",
        "        \n",
        "        # Draw the bounding box on the original image\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
        "        \n",
        "        # Get the class name from the label\n",
        "        class_name = class_names[labels.detach().numpy()[i] - 1]\n",
        "        classes.append(class_name)\n",
        "        \n",
        "        # Calculate and round the confidence score\n",
        "        conf_score = round(scores.detach().numpy()[i] * 100)\n",
        "        conf_scores.append(conf_score)\n",
        "        \n",
        "        # Put text with the class name on the image\n",
        "        cv2.putText(img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "    # Append results of the current frame to the results dictionary\n",
        "    results_fr['bboxes'].append(boxes)\n",
        "    results_fr['conf_scores'].append(conf_scores)\n",
        "    results_fr['class_names'].append(classes)\n",
        "    results_fr['inference_time'].append(end_time - start_time)\n",
        "\n",
        "    # Display the frame with bounding boxes and labels\n",
        "    cv2.imshow('Frame', img)\n",
        "\n",
        "    # Break the loop if 'c' or 'C' is pressed\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    if key == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the video capture object and close all OpenCV windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "4flzeignrChG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\MSI\\anaconda3\\envs\\imageD\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
            "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
          ]
        }
      ],
      "source": [
        "cap = cv2.VideoCapture(\"video.mp4\")\n",
        "\n",
        "results_s = {'inference_time' : [],\n",
        "           'conf_scores' : [],\n",
        "           'class_names' : [],\n",
        "           'bboxes' : []}\n",
        "\n",
        "i = 0\n",
        "\n",
        "while True:\n",
        "  success, img = cap.read()\n",
        "\n",
        "  if not success:\n",
        "    break\n",
        "\n",
        "  transformed_img = transform(img)\n",
        "\n",
        "  start_time = time()\n",
        "  pred = ssd([transformed_img])\n",
        "  end_time = time()\n",
        "\n",
        "  bboxes, labels, scores = pred[0]['boxes'], pred[0]['labels'], pred[0]['scores']\n",
        "\n",
        "  num = torch.argwhere(scores > 0.8).shape[0]\n",
        "\n",
        "  classes = []\n",
        "  boxes = []\n",
        "  conf_scores = []\n",
        "\n",
        "  for i in range(num):\n",
        "    x1, y1, x2, y2 = bboxes[i].detach().numpy().astype('int')\n",
        "    boxes.append([x1, y1, x2, y2])\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
        "    class_name = class_names[labels.detach().numpy()[i] - 1]\n",
        "    classes.append(class_name)\n",
        "    conf_score = round(scores.detach().numpy()[i] * 100)\n",
        "    conf_scores.append(conf_score)\n",
        "    cv2.putText(img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "  results_s['bboxes'].append(boxes)\n",
        "  results_s['conf_scores'].append(conf_scores)\n",
        "  results_s['class_names'].append(classes)\n",
        "  results_s['inference_time'].append(end_time - start_time)\n",
        "\n",
        "  cv2.imshow('Frame', img)\n",
        "\n",
        "  key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "  if key == ord('q'):\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "def yolo_transform(img):\n",
        "    # Convert the image from BGR (OpenCV default) to RGB color space\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Convert the RGB image to a PyTorch tensor and normalize pixel values to the range [0, 1]\n",
        "    img_tensor = torch.tensor(img_rgb).float() / 255.0\n",
        "    \n",
        "    # Rearrange the tensor dimensions from (H, W, C) to (C, H, W) as required by PyTorch models\n",
        "    img_tensor = img_tensor.permute(2, 0, 1)\n",
        "    \n",
        "    return img_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 352x640 6 persons, 1 boat, 1 backpack, 154.4ms\n",
            "Speed: 708.1ms preprocess, 154.4ms inference, 7.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 6 persons, 1 boat, 1 backpack, 1 handbag, 69.6ms\n",
            "Speed: 2.0ms preprocess, 69.6ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 1 handbag, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 6 persons, 1 boat, 1 handbag, 17.7ms\n",
            "Speed: 0.7ms preprocess, 17.7ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 19.2ms\n",
            "Speed: 1.0ms preprocess, 19.2ms inference, 7.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 19.0ms\n",
            "Speed: 1.0ms preprocess, 19.0ms inference, 7.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 18.7ms\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 8.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 32.7ms\n",
            "Speed: 0.0ms preprocess, 32.7ms inference, 7.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 17.8ms\n",
            "Speed: 0.6ms preprocess, 17.8ms inference, 6.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 20.2ms\n",
            "Speed: 1.1ms preprocess, 20.2ms inference, 6.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 8 persons, 2 boats, 18.6ms\n",
            "Speed: 1.0ms preprocess, 18.6ms inference, 5.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 8 persons, 2 boats, 18.0ms\n",
            "Speed: 0.0ms preprocess, 18.0ms inference, 6.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 8 persons, 1 boat, 18.5ms\n",
            "Speed: 0.6ms preprocess, 18.5ms inference, 6.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 31.3ms\n",
            "Speed: 1.0ms preprocess, 31.3ms inference, 8.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 16.4ms\n",
            "Speed: 0.8ms preprocess, 16.4ms inference, 6.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 18.1ms\n",
            "Speed: 0.0ms preprocess, 18.1ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 8 persons, 15.9ms\n",
            "Speed: 1.0ms preprocess, 15.9ms inference, 6.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 16.1ms\n",
            "Speed: 1.0ms preprocess, 16.1ms inference, 7.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 16.1ms\n",
            "Speed: 0.9ms preprocess, 16.1ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 18.5ms\n",
            "Speed: 1.5ms preprocess, 18.5ms inference, 9.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 8 persons, 30.4ms\n",
            "Speed: 0.5ms preprocess, 30.4ms inference, 7.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 5.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 boat, 26.8ms\n",
            "Speed: 0.6ms preprocess, 26.8ms inference, 6.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 16.1ms\n",
            "Speed: 0.7ms preprocess, 16.1ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 18.9ms\n",
            "Speed: 1.0ms preprocess, 18.9ms inference, 6.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 18.0ms\n",
            "Speed: 1.0ms preprocess, 18.0ms inference, 6.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 skateboard, 17.3ms\n",
            "Speed: 0.0ms preprocess, 17.3ms inference, 5.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 16.0ms\n",
            "Speed: 0.9ms preprocess, 16.0ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 16.1ms\n",
            "Speed: 1.0ms preprocess, 16.1ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 17.0ms\n",
            "Speed: 0.9ms preprocess, 17.0ms inference, 7.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 17.1ms\n",
            "Speed: 1.2ms preprocess, 17.1ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 16.2ms\n",
            "Speed: 0.8ms preprocess, 16.2ms inference, 7.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 16.2ms\n",
            "Speed: 1.0ms preprocess, 16.2ms inference, 6.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 19.3ms\n",
            "Speed: 0.0ms preprocess, 19.3ms inference, 6.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 18.8ms\n",
            "Speed: 1.0ms preprocess, 18.8ms inference, 7.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 18.7ms\n",
            "Speed: 1.0ms preprocess, 18.7ms inference, 7.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 15.1ms\n",
            "Speed: 1.0ms preprocess, 15.1ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 16.0ms\n",
            "Speed: 0.0ms preprocess, 16.0ms inference, 5.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 suitcase, 16.0ms\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 15.3ms\n",
            "Speed: 1.0ms preprocess, 15.3ms inference, 5.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 15.2ms\n",
            "Speed: 0.0ms preprocess, 15.2ms inference, 4.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 14.4ms\n",
            "Speed: 0.0ms preprocess, 14.4ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 15.5ms\n",
            "Speed: 0.0ms preprocess, 15.5ms inference, 5.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 18.2ms\n",
            "Speed: 0.0ms preprocess, 18.2ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 16.1ms\n",
            "Speed: 1.0ms preprocess, 16.1ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 16.9ms\n",
            "Speed: 0.0ms preprocess, 16.9ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 15.2ms\n",
            "Speed: 1.0ms preprocess, 15.2ms inference, 5.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 15.7ms\n",
            "Speed: 1.0ms preprocess, 15.7ms inference, 5.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 19.1ms\n",
            "Speed: 1.0ms preprocess, 19.1ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 16.3ms\n",
            "Speed: 0.0ms preprocess, 16.3ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 16.1ms\n",
            "Speed: 1.0ms preprocess, 16.1ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 17.0ms\n",
            "Speed: 0.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 16.6ms\n",
            "Speed: 0.0ms preprocess, 16.6ms inference, 5.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 16.3ms\n",
            "Speed: 1.0ms preprocess, 16.3ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 17.8ms\n",
            "Speed: 0.0ms preprocess, 17.8ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 25.5ms\n",
            "Speed: 1.0ms preprocess, 25.5ms inference, 6.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 17.5ms\n",
            "Speed: 0.0ms preprocess, 17.5ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 24.0ms\n",
            "Speed: 0.0ms preprocess, 24.0ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 18.2ms\n",
            "Speed: 1.0ms preprocess, 18.2ms inference, 3.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 19.9ms\n",
            "Speed: 0.0ms preprocess, 19.9ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 16.3ms\n",
            "Speed: 1.0ms preprocess, 16.3ms inference, 5.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 15 persons, 16.0ms\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 16.0ms\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 bench, 16.0ms\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 boat, 1 handbag, 17.4ms\n",
            "Speed: 0.0ms preprocess, 17.4ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 boat, 1 handbag, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 7.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 2 boats, 1 handbag, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 4.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 2 boats, 16.7ms\n",
            "Speed: 0.0ms preprocess, 16.7ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 2 boats, 1 handbag, 16.9ms\n",
            "Speed: 0.0ms preprocess, 16.9ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 2 boats, 16.7ms\n",
            "Speed: 0.0ms preprocess, 16.7ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 2 boats, 1 handbag, 17.0ms\n",
            "Speed: 0.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 boat, 16.6ms\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 boat, 1 handbag, 16.7ms\n",
            "Speed: 0.5ms preprocess, 16.7ms inference, 5.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 boat, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 boat, 1 handbag, 20.2ms\n",
            "Speed: 1.0ms preprocess, 20.2ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 boat, 1 handbag, 1 suitcase, 17.0ms\n",
            "Speed: 0.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 boat, 1 backpack, 1 handbag, 1 suitcase, 17.5ms\n",
            "Speed: 1.4ms preprocess, 17.5ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 boat, 2 handbags, 1 suitcase, 16.5ms\n",
            "Speed: 0.0ms preprocess, 16.5ms inference, 7.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 2 boats, 2 handbags, 1 suitcase, 17.1ms\n",
            "Speed: 1.2ms preprocess, 17.1ms inference, 74.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 boat, 1 handbag, 22.0ms\n",
            "Speed: 1.0ms preprocess, 22.0ms inference, 52.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 boat, 1 handbag, 1 skateboard, 16.7ms\n",
            "Speed: 0.6ms preprocess, 16.7ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 handbag, 16.6ms\n",
            "Speed: 0.0ms preprocess, 16.6ms inference, 9.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 boat, 1 handbag, 17.6ms\n",
            "Speed: 1.2ms preprocess, 17.6ms inference, 37.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 boat, 1 handbag, 27.0ms\n",
            "Speed: 0.6ms preprocess, 27.0ms inference, 8.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 boat, 1 handbag, 17.6ms\n",
            "Speed: 0.0ms preprocess, 17.6ms inference, 6.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 boat, 1 handbag, 16.7ms\n",
            "Speed: 1.1ms preprocess, 16.7ms inference, 6.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 boat, 18.9ms\n",
            "Speed: 1.0ms preprocess, 18.9ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 16.5ms\n",
            "Speed: 0.0ms preprocess, 16.5ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 21.5ms\n",
            "Speed: 1.0ms preprocess, 21.5ms inference, 6.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 8 persons, 1 bench, 1 suitcase, 22.8ms\n",
            "Speed: 0.0ms preprocess, 22.8ms inference, 7.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 bench, 2 suitcases, 21.7ms\n",
            "Speed: 1.0ms preprocess, 21.7ms inference, 8.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 2 handbags, 19.5ms\n",
            "Speed: 1.0ms preprocess, 19.5ms inference, 9.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 2 handbags, 22.3ms\n",
            "Speed: 1.0ms preprocess, 22.3ms inference, 23.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 20.0ms\n",
            "Speed: 1.0ms preprocess, 20.0ms inference, 32.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 backpack, 1 handbag, 23.0ms\n",
            "Speed: 0.0ms preprocess, 23.0ms inference, 44.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 backpack, 1 handbag, 17.9ms\n",
            "Speed: 0.0ms preprocess, 17.9ms inference, 58.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 handbag, 19.7ms\n",
            "Speed: 1.0ms preprocess, 19.7ms inference, 53.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 2 backpacks, 1 handbag, 22.2ms\n",
            "Speed: 1.0ms preprocess, 22.2ms inference, 32.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 backpack, 21.2ms\n",
            "Speed: 1.0ms preprocess, 21.2ms inference, 20.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 24.8ms\n",
            "Speed: 1.0ms preprocess, 24.8ms inference, 9.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 22.9ms\n",
            "Speed: 1.0ms preprocess, 22.9ms inference, 6.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 22.9ms\n",
            "Speed: 1.1ms preprocess, 22.9ms inference, 7.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 skateboard, 22.4ms\n",
            "Speed: 1.1ms preprocess, 22.4ms inference, 8.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 25.6ms\n",
            "Speed: 1.0ms preprocess, 25.6ms inference, 8.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 23.7ms\n",
            "Speed: 1.0ms preprocess, 23.7ms inference, 5.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 19.6ms\n",
            "Speed: 1.0ms preprocess, 19.6ms inference, 6.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 20.0ms\n",
            "Speed: 1.0ms preprocess, 20.0ms inference, 6.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 25.0ms\n",
            "Speed: 1.0ms preprocess, 25.0ms inference, 7.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 suitcase, 20.4ms\n",
            "Speed: 1.1ms preprocess, 20.4ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 suitcase, 20.2ms\n",
            "Speed: 0.5ms preprocess, 20.2ms inference, 5.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 suitcase, 20.0ms\n",
            "Speed: 1.6ms preprocess, 20.0ms inference, 5.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 27.7ms\n",
            "Speed: 1.1ms preprocess, 27.7ms inference, 6.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 20.0ms\n",
            "Speed: 0.5ms preprocess, 20.0ms inference, 5.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 backpack, 20.1ms\n",
            "Speed: 1.0ms preprocess, 20.1ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 backpack, 19.3ms\n",
            "Speed: 1.0ms preprocess, 19.3ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 backpack, 20.0ms\n",
            "Speed: 1.1ms preprocess, 20.0ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 2 backpacks, 20.6ms\n",
            "Speed: 0.0ms preprocess, 20.6ms inference, 5.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 2 backpacks, 1 handbag, 20.1ms\n",
            "Speed: 1.1ms preprocess, 20.1ms inference, 5.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 backpack, 1 handbag, 22.4ms\n",
            "Speed: 1.2ms preprocess, 22.4ms inference, 7.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 18.0ms\n",
            "Speed: 1.1ms preprocess, 18.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 18.4ms\n",
            "Speed: 0.0ms preprocess, 18.4ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 20.4ms\n",
            "Speed: 1.0ms preprocess, 20.4ms inference, 6.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 25.1ms\n",
            "Speed: 0.0ms preprocess, 25.1ms inference, 6.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 20.7ms\n",
            "Speed: 1.0ms preprocess, 20.7ms inference, 7.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 17.9ms\n",
            "Speed: 1.1ms preprocess, 17.9ms inference, 6.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 suitcase, 17.8ms\n",
            "Speed: 0.0ms preprocess, 17.8ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 2 suitcases, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 suitcase, 17.1ms\n",
            "Speed: 1.1ms preprocess, 17.1ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 backpack, 1 chair, 18.0ms\n",
            "Speed: 0.0ms preprocess, 18.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 suitcase, 18.8ms\n",
            "Speed: 0.0ms preprocess, 18.8ms inference, 6.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 suitcase, 17.1ms\n",
            "Speed: 1.1ms preprocess, 17.1ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 backpack, 1 skateboard, 16.5ms\n",
            "Speed: 1.0ms preprocess, 16.5ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 19.7ms\n",
            "Speed: 0.0ms preprocess, 19.7ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 17.5ms\n",
            "Speed: 0.0ms preprocess, 17.5ms inference, 6.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 backpack, 1 suitcase, 18.0ms\n",
            "Speed: 1.0ms preprocess, 18.0ms inference, 5.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 14 persons, 1 backpack, 1 suitcase, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 5.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 14 persons, 1 backpack, 22.7ms\n",
            "Speed: 0.0ms preprocess, 22.7ms inference, 6.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 14 persons, 1 backpack, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 backpack, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 6.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 14 persons, 1 backpack, 1 suitcase, 17.3ms\n",
            "Speed: 1.0ms preprocess, 17.3ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 backpack, 22.4ms\n",
            "Speed: 1.0ms preprocess, 22.4ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 backpack, 20.1ms\n",
            "Speed: 1.1ms preprocess, 20.1ms inference, 6.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 17.7ms\n",
            "Speed: 0.0ms preprocess, 17.7ms inference, 6.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 backpack, 17.3ms\n",
            "Speed: 1.2ms preprocess, 17.3ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 14 persons, 1 suitcase, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 suitcase, 19.7ms\n",
            "Speed: 1.0ms preprocess, 19.7ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 14 persons, 18.0ms\n",
            "Speed: 0.0ms preprocess, 18.0ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 4.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 17.0ms\n",
            "Speed: 0.0ms preprocess, 17.0ms inference, 7.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 suitcase, 17.4ms\n",
            "Speed: 1.0ms preprocess, 17.4ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 suitcase, 17.3ms\n",
            "Speed: 1.0ms preprocess, 17.3ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 17.2ms\n",
            "Speed: 1.1ms preprocess, 17.2ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 17.7ms\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 5.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 17.1ms\n",
            "Speed: 0.9ms preprocess, 17.1ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 boat, 17.7ms\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 5.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 24.3ms\n",
            "Speed: 1.0ms preprocess, 24.3ms inference, 7.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 handbag, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 24.2ms\n",
            "Speed: 0.8ms preprocess, 24.2ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 16.2ms\n",
            "Speed: 1.0ms preprocess, 16.2ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 17.2ms\n",
            "Speed: 0.0ms preprocess, 17.2ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 19.0ms\n",
            "Speed: 0.7ms preprocess, 19.0ms inference, 7.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 16.5ms\n",
            "Speed: 1.0ms preprocess, 16.5ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 20.5ms\n",
            "Speed: 1.0ms preprocess, 20.5ms inference, 5.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 handbag, 1 skateboard, 23.3ms\n",
            "Speed: 1.0ms preprocess, 23.3ms inference, 7.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 17.0ms\n",
            "Speed: 0.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 17.7ms\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 6.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 17.1ms\n",
            "Speed: 0.0ms preprocess, 17.1ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 16.0ms\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 17.6ms\n",
            "Speed: 1.0ms preprocess, 17.6ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 16.2ms\n",
            "Speed: 1.0ms preprocess, 16.2ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 backpack, 19.2ms\n",
            "Speed: 1.0ms preprocess, 19.2ms inference, 6.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 backpack, 1 suitcase, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 2 backpacks, 1 handbag, 16.8ms\n",
            "Speed: 1.1ms preprocess, 16.8ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 2 backpacks, 1 suitcase, 17.8ms\n",
            "Speed: 0.0ms preprocess, 17.8ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 2 backpacks, 1 suitcase, 18.0ms\n",
            "Speed: 1.0ms preprocess, 18.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 backpack, 1 suitcase, 17.3ms\n",
            "Speed: 0.0ms preprocess, 17.3ms inference, 6.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 5.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 backpack, 17.3ms\n",
            "Speed: 1.0ms preprocess, 17.3ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 backpack, 17.5ms\n",
            "Speed: 0.0ms preprocess, 17.5ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 23.0ms\n",
            "Speed: 1.0ms preprocess, 23.0ms inference, 6.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 handbag, 17.9ms\n",
            "Speed: 0.0ms preprocess, 17.9ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 handbag, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 4.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 handbag, 17.8ms\n",
            "Speed: 0.0ms preprocess, 17.8ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 2 handbags, 17.5ms\n",
            "Speed: 1.0ms preprocess, 17.5ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 backpack, 1 handbag, 17.7ms\n",
            "Speed: 1.0ms preprocess, 17.7ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 2 backpacks, 1 handbag, 1 skateboard, 20.0ms\n",
            "Speed: 1.0ms preprocess, 20.0ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 2 backpacks, 1 handbag, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 2 backpacks, 16.6ms\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 2 backpacks, 1 handbag, 16.2ms\n",
            "Speed: 1.0ms preprocess, 16.2ms inference, 5.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 backpack, 1 handbag, 16.4ms\n",
            "Speed: 1.0ms preprocess, 16.4ms inference, 5.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 14 persons, 1 backpack, 16.2ms\n",
            "Speed: 0.0ms preprocess, 16.2ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 14 persons, 1 backpack, 1 handbag, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 16 persons, 1 backpack, 2 handbags, 15.7ms\n",
            "Speed: 1.3ms preprocess, 15.7ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 14 persons, 2 backpacks, 1 handbag, 17.0ms\n",
            "Speed: 0.0ms preprocess, 17.0ms inference, 3.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 14 persons, 2 backpacks, 2 handbags, 16.9ms\n",
            "Speed: 0.0ms preprocess, 16.9ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 backpack, 1 handbag, 15.9ms\n",
            "Speed: 1.0ms preprocess, 15.9ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 2 backpacks, 2 handbags, 16.4ms\n",
            "Speed: 1.0ms preprocess, 16.4ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 2 backpacks, 1 handbag, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 5.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 2 backpacks, 2 handbags, 1 suitcase, 1 skateboard, 15.8ms\n",
            "Speed: 0.9ms preprocess, 15.8ms inference, 5.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 2 backpacks, 2 handbags, 1 skateboard, 16.5ms\n",
            "Speed: 1.0ms preprocess, 16.5ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 3 backpacks, 1 handbag, 17.2ms\n",
            "Speed: 0.9ms preprocess, 17.2ms inference, 5.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 2 backpacks, 2 handbags, 16.5ms\n",
            "Speed: 0.0ms preprocess, 16.5ms inference, 6.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 3 backpacks, 2 handbags, 16.2ms\n",
            "Speed: 1.0ms preprocess, 16.2ms inference, 4.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 3 backpacks, 1 handbag, 16.7ms\n",
            "Speed: 0.0ms preprocess, 16.7ms inference, 5.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 backpack, 2 handbags, 1 suitcase, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 5.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 2 backpacks, 1 handbag, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 2 backpacks, 1 handbag, 19.9ms\n",
            "Speed: 0.7ms preprocess, 19.9ms inference, 5.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 backpack, 2 handbags, 17.3ms\n",
            "Speed: 0.0ms preprocess, 17.3ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 2 backpacks, 1 handbag, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 5.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 backpack, 2 handbags, 16.5ms\n",
            "Speed: 1.0ms preprocess, 16.5ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 backpack, 1 handbag, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 2 backpacks, 16.2ms\n",
            "Speed: 1.4ms preprocess, 16.2ms inference, 6.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 1 backpack, 1 handbag, 16.2ms\n",
            "Speed: 1.1ms preprocess, 16.2ms inference, 7.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 backpack, 20.3ms\n",
            "Speed: 1.0ms preprocess, 20.3ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 backpack, 1 handbag, 16.2ms\n",
            "Speed: 1.0ms preprocess, 16.2ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 16.2ms\n",
            "Speed: 1.0ms preprocess, 16.2ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 17.9ms\n",
            "Speed: 1.0ms preprocess, 17.9ms inference, 5.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 16.3ms\n",
            "Speed: 0.0ms preprocess, 16.3ms inference, 5.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 skateboard, 16.0ms\n",
            "Speed: 1.1ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 backpack, 16.9ms\n",
            "Speed: 0.0ms preprocess, 16.9ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 backpack, 16.7ms\n",
            "Speed: 0.0ms preprocess, 16.7ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 backpack, 16.4ms\n",
            "Speed: 1.0ms preprocess, 16.4ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 backpack, 16.4ms\n",
            "Speed: 1.1ms preprocess, 16.4ms inference, 5.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 handbag, 17.1ms\n",
            "Speed: 0.0ms preprocess, 17.1ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 19.3ms\n",
            "Speed: 1.1ms preprocess, 19.3ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 skateboard, 15.6ms\n",
            "Speed: 1.0ms preprocess, 15.6ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 skateboard, 17.7ms\n",
            "Speed: 1.5ms preprocess, 17.7ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 17.3ms\n",
            "Speed: 1.0ms preprocess, 17.3ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 12 persons, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 16.3ms\n",
            "Speed: 0.8ms preprocess, 16.3ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 16.6ms\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 2 backpacks, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 backpack, 16.0ms\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 2 backpacks, 17.0ms\n",
            "Speed: 1.2ms preprocess, 17.0ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 2 backpacks, 16.6ms\n",
            "Speed: 0.0ms preprocess, 16.6ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 2 backpacks, 1 suitcase, 20.0ms\n",
            "Speed: 1.0ms preprocess, 20.0ms inference, 5.6ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 backpack, 17.2ms\n",
            "Speed: 0.0ms preprocess, 17.2ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 backpack, 1 skateboard, 20.1ms\n",
            "Speed: 1.0ms preprocess, 20.1ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 2 backpacks, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 5.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 2 backpacks, 16.0ms\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 backpack, 1 handbag, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 backpack, 16.9ms\n",
            "Speed: 0.0ms preprocess, 16.9ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 backpack, 23.6ms\n",
            "Speed: 0.0ms preprocess, 23.6ms inference, 6.2ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 2 backpacks, 16.2ms\n",
            "Speed: 1.0ms preprocess, 16.2ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 2 backpacks, 17.1ms\n",
            "Speed: 0.0ms preprocess, 17.1ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 2 backpacks, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 backpack, 1 handbag, 16.8ms\n",
            "Speed: 0.0ms preprocess, 16.8ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 backpack, 1 handbag, 16.9ms\n",
            "Speed: 1.0ms preprocess, 16.9ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 3 backpacks, 1 handbag, 15.6ms\n",
            "Speed: 1.0ms preprocess, 15.6ms inference, 5.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 3 backpacks, 2 handbags, 17.1ms\n",
            "Speed: 1.0ms preprocess, 17.1ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 2 backpacks, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 13 persons, 1 backpack, 1 handbag, 17.3ms\n",
            "Speed: 0.0ms preprocess, 17.3ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 11 persons, 1 backpack, 2 handbags, 17.2ms\n",
            "Speed: 1.0ms preprocess, 17.2ms inference, 4.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 backpack, 3 handbags, 16.0ms\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 1 backpack, 16.0ms\n",
            "Speed: 1.0ms preprocess, 16.0ms inference, 5.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 10 persons, 2 backpacks, 1 skateboard, 16.0ms\n",
            "Speed: 0.5ms preprocess, 16.0ms inference, 6.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 boat, 2 backpacks, 17.1ms\n",
            "Speed: 0.0ms preprocess, 17.1ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 backpack, 16.7ms\n",
            "Speed: 0.7ms preprocess, 16.7ms inference, 5.3ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 backpack, 1 handbag, 16.1ms\n",
            "Speed: 1.0ms preprocess, 16.1ms inference, 4.8ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 backpack, 16.6ms\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 8 persons, 2 backpacks, 19.1ms\n",
            "Speed: 1.0ms preprocess, 19.1ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 8 persons, 1 backpack, 17.0ms\n",
            "Speed: 0.0ms preprocess, 17.0ms inference, 3.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 backpack, 1 handbag, 16.6ms\n",
            "Speed: 1.0ms preprocess, 16.6ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 2 backpacks, 1 handbag, 17.0ms\n",
            "Speed: 1.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 2 backpacks, 1 handbag, 1 skateboard, 19.2ms\n",
            "Speed: 1.0ms preprocess, 19.2ms inference, 5.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 9 persons, 1 backpack, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 7.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 8 persons, 1 backpack, 17.3ms\n",
            "Speed: 0.0ms preprocess, 17.3ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 8 persons, 16.7ms\n",
            "Speed: 0.0ms preprocess, 16.7ms inference, 6.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 2 backpacks, 17.1ms\n",
            "Speed: 0.0ms preprocess, 17.1ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 2 backpacks, 15.7ms\n",
            "Speed: 1.0ms preprocess, 15.7ms inference, 5.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 8 persons, 1 backpack, 16.0ms\n",
            "Speed: 1.1ms preprocess, 16.0ms inference, 4.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 2 backpacks, 16.7ms\n",
            "Speed: 1.0ms preprocess, 16.7ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 2 backpacks, 15.9ms\n",
            "Speed: 0.8ms preprocess, 15.9ms inference, 5.9ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 1 backpack, 20.4ms\n",
            "Speed: 1.1ms preprocess, 20.4ms inference, 5.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 352x640 7 persons, 1 boat, 1 backpack, 16.8ms\n",
            "Speed: 1.0ms preprocess, 16.8ms inference, 5.0ms postprocess per image at shape (1, 3, 352, 640)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms.functional as F\n",
        "from time import time\n",
        "\n",
        "# Open the video file for processing\n",
        "cap = cv2.VideoCapture(\"video.mp4\")\n",
        "\n",
        "# Initialize a dictionary to store results\n",
        "results_y = {\n",
        "    'inference_time': [],  # To store the time taken for inference\n",
        "    'conf_scores': [],     # To store confidence scores of detections\n",
        "    'class_names': [],     # To store class names for each detection\n",
        "    'bboxes': []           # To store bounding boxes of detections\n",
        "}\n",
        "\n",
        "# Counter for frames\n",
        "i = 0\n",
        "\n",
        "while True:\n",
        "    # Read a frame from the video\n",
        "    success, img = cap.read()\n",
        "\n",
        "    # Break the loop if no frame is captured (end of video)\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    # Transform the frame (e.g., resize and normalize)\n",
        "    transformed_img = transform(img)\n",
        "\n",
        "    # Resize the image dimensions to be multiples of 32 for YOLO\n",
        "    new_height = (transformed_img.shape[1] // 32) * 32\n",
        "    new_width = (transformed_img.shape[2] // 32) * 32\n",
        "    \n",
        "    # Use bilinear interpolation to resize the image\n",
        "    transformed_img_resized = F.interpolate(\n",
        "        transformed_img.unsqueeze(dim=0), \n",
        "        size=(new_height, new_width), \n",
        "        mode='bilinear', \n",
        "        align_corners=False\n",
        "    )\n",
        "\n",
        "    # Start timing the inference process\n",
        "    start_time = time()\n",
        "    \n",
        "    # Perform object detection on the resized image\n",
        "    preds = yolo(transformed_img_resized)\n",
        "    \n",
        "    # End timing the inference process\n",
        "    end_time = time()\n",
        "\n",
        "    # Initialize lists to store results for the current frame\n",
        "    classes = []\n",
        "    boxes = []\n",
        "    conf_scores = []\n",
        "\n",
        "    # Process each prediction\n",
        "    for pred in preds:\n",
        "        # Extract bounding boxes from the prediction\n",
        "        box = pred.boxes\n",
        "        \n",
        "        for b in box:\n",
        "            # Extract confidence score and convert to percentage\n",
        "            conf_score = round(b.conf[0].cpu().numpy() * 100)\n",
        "            \n",
        "            # Filter out predictions with low confidence\n",
        "            if conf_score > 80:\n",
        "                # Extract bounding box coordinates\n",
        "                x1, y1, x2, y2 = b.xyxy[0].cpu().numpy().astype('int')\n",
        "                boxes.append([x1, y1, x2, y2])\n",
        "                \n",
        "                # Draw the bounding box on the original image\n",
        "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
        "                \n",
        "                # Append confidence score and class name to lists\n",
        "                conf_scores.append(conf_score)\n",
        "                class_name = class_names[int(b.cls[0].cpu().numpy())]\n",
        "                classes.append(class_name)\n",
        "                \n",
        "                # Put text with the class name on the image\n",
        "                cv2.putText(img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "    # Append results of the current frame to the results dictionary\n",
        "    results_y['bboxes'].append(boxes)\n",
        "    results_y['conf_scores'].append(conf_scores)\n",
        "    results_y['class_names'].append(classes)\n",
        "    results_y['inference_time'].append(end_time - start_time)\n",
        "\n",
        "    # Display the frame with bounding boxes and labels\n",
        "    cv2.imshow('Frame', img)\n",
        "\n",
        "    # Break the loop if 'c' or 'C' is pressed\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    if key == ord('c') or key == ord('C'):\n",
        "        break\n",
        "\n",
        "# Release the video capture object and close all OpenCV windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inference_time</th>\n",
              "      <th>conf_scores</th>\n",
              "      <th>class_names</th>\n",
              "      <th>bboxes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>0.026670</td>\n",
              "      <td>[90, 84]</td>\n",
              "      <td>[person, person]</td>\n",
              "      <td>[[598, 146, 639, 254], [262, 156, 288, 262]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>0.028099</td>\n",
              "      <td>[87, 84, 82]</td>\n",
              "      <td>[person, person, person]</td>\n",
              "      <td>[[598, 146, 638, 253], [255, 156, 287, 261], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>0.027832</td>\n",
              "      <td>[86, 85, 83]</td>\n",
              "      <td>[person, person, person]</td>\n",
              "      <td>[[597, 145, 633, 254], [248, 156, 287, 261], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>0.031247</td>\n",
              "      <td>[86, 84, 81]</td>\n",
              "      <td>[person, person, person]</td>\n",
              "      <td>[[242, 156, 287, 261], [536, 153, 575, 253], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>0.027755</td>\n",
              "      <td>[87, 83, 81]</td>\n",
              "      <td>[person, person, person]</td>\n",
              "      <td>[[239, 156, 287, 261], [591, 145, 623, 255], [...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     inference_time   conf_scores               class_names  \\\n",
              "276        0.026670      [90, 84]          [person, person]   \n",
              "277        0.028099  [87, 84, 82]  [person, person, person]   \n",
              "278        0.027832  [86, 85, 83]  [person, person, person]   \n",
              "279        0.031247  [86, 84, 81]  [person, person, person]   \n",
              "280        0.027755  [87, 83, 81]  [person, person, person]   \n",
              "\n",
              "                                                bboxes  \n",
              "276       [[598, 146, 639, 254], [262, 156, 288, 262]]  \n",
              "277  [[598, 146, 638, 253], [255, 156, 287, 261], [...  \n",
              "278  [[597, 145, 633, 254], [248, 156, 287, 261], [...  \n",
              "279  [[242, 156, 287, 261], [536, 153, 575, 253], [...  \n",
              "280  [[239, 156, 287, 261], [591, 145, 623, 255], [...  "
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_yolo_df = pd.DataFrame(results_y) # dataframe for yolo results\n",
        "results_yolo_df.tail() # last 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average inference time YOLO: 0.033976564203717106\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average inference time YOLO: {results_yolo_df['inference_time'].mean()}\") # average inference time yolo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inference_time</th>\n",
              "      <th>conf_scores</th>\n",
              "      <th>class_names</th>\n",
              "      <th>bboxes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>3.998757</td>\n",
              "      <td>[100, 100, 100, 100, 99, 99, 91, 87]</td>\n",
              "      <td>[person, person, person, person, person, perso...</td>\n",
              "      <td>[[598, 150, 635, 260], [261, 158, 288, 267], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>3.638347</td>\n",
              "      <td>[100, 100, 100, 99, 99, 98, 86]</td>\n",
              "      <td>[person, person, person, person, person, perso...</td>\n",
              "      <td>[[259, 158, 288, 267], [598, 149, 633, 261], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>3.889045</td>\n",
              "      <td>[100, 100, 100, 100, 100, 99, 89]</td>\n",
              "      <td>[person, person, person, person, person, perso...</td>\n",
              "      <td>[[597, 149, 631, 261], [201, 176, 235, 266], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>2.614484</td>\n",
              "      <td>[100, 100, 100, 100, 99, 99]</td>\n",
              "      <td>[person, person, person, person, person, person]</td>\n",
              "      <td>[[595, 149, 627, 260], [200, 175, 233, 267], [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>3.239111</td>\n",
              "      <td>[100, 100, 100, 99, 99, 97]</td>\n",
              "      <td>[person, person, person, person, person, person]</td>\n",
              "      <td>[[594, 148, 624, 260], [536, 156, 571, 259], [...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     inference_time                           conf_scores  \\\n",
              "276        3.998757  [100, 100, 100, 100, 99, 99, 91, 87]   \n",
              "277        3.638347       [100, 100, 100, 99, 99, 98, 86]   \n",
              "278        3.889045     [100, 100, 100, 100, 100, 99, 89]   \n",
              "279        2.614484          [100, 100, 100, 100, 99, 99]   \n",
              "280        3.239111           [100, 100, 100, 99, 99, 97]   \n",
              "\n",
              "                                           class_names  \\\n",
              "276  [person, person, person, person, person, perso...   \n",
              "277  [person, person, person, person, person, perso...   \n",
              "278  [person, person, person, person, person, perso...   \n",
              "279   [person, person, person, person, person, person]   \n",
              "280   [person, person, person, person, person, person]   \n",
              "\n",
              "                                                bboxes  \n",
              "276  [[598, 150, 635, 260], [261, 158, 288, 267], [...  \n",
              "277  [[259, 158, 288, 267], [598, 149, 633, 261], [...  \n",
              "278  [[597, 149, 631, 261], [201, 176, 235, 266], [...  \n",
              "279  [[595, 149, 627, 260], [200, 175, 233, 267], [...  \n",
              "280  [[594, 148, 624, 260], [536, 156, 571, 259], [...  "
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_fr_df = pd.DataFrame(results_fr) # dataframe for faster rcnn\n",
        "results_fr_df.tail() # last 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average inference time Faster RCNN: 2.606488109059181\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average inference time Faster RCNN: {results_fr_df['inference_time'].mean()}\") # average inference time faster rcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inference_time</th>\n",
              "      <th>conf_scores</th>\n",
              "      <th>class_names</th>\n",
              "      <th>bboxes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>0.407242</td>\n",
              "      <td>[99, 89]</td>\n",
              "      <td>[person, person]</td>\n",
              "      <td>[[543, 160, 584, 261], [203, 174, 240, 268]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>0.226633</td>\n",
              "      <td>[98, 90]</td>\n",
              "      <td>[person, person]</td>\n",
              "      <td>[[540, 163, 581, 261], [202, 173, 239, 268]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>0.437428</td>\n",
              "      <td>[97, 87]</td>\n",
              "      <td>[person, person]</td>\n",
              "      <td>[[539, 162, 579, 262], [201, 173, 238, 268]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>0.416263</td>\n",
              "      <td>[95, 90]</td>\n",
              "      <td>[person, person]</td>\n",
              "      <td>[[537, 159, 576, 263], [589, 150, 627, 257]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>0.315461</td>\n",
              "      <td>[94, 93, 91]</td>\n",
              "      <td>[person, person, person]</td>\n",
              "      <td>[[587, 150, 624, 258], [536, 158, 572, 262], [...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     inference_time   conf_scores               class_names  \\\n",
              "276        0.407242      [99, 89]          [person, person]   \n",
              "277        0.226633      [98, 90]          [person, person]   \n",
              "278        0.437428      [97, 87]          [person, person]   \n",
              "279        0.416263      [95, 90]          [person, person]   \n",
              "280        0.315461  [94, 93, 91]  [person, person, person]   \n",
              "\n",
              "                                                bboxes  \n",
              "276       [[543, 160, 584, 261], [203, 174, 240, 268]]  \n",
              "277       [[540, 163, 581, 261], [202, 173, 239, 268]]  \n",
              "278       [[539, 162, 579, 262], [201, 173, 238, 268]]  \n",
              "279       [[537, 159, 576, 263], [589, 150, 627, 257]]  \n",
              "280  [[587, 150, 624, 258], [536, 158, 572, 262], [...  "
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_s_df = pd.DataFrame(results_s) # dataframe for ssd\n",
        "results_s_df.tail() # last 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average inference time SSD: 0.39548616052946584\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average inference time SSD: {results_s_df['inference_time'].mean()}\") # average inference time ssd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SDQfG4jVwk_"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM1DEyVoVx3B"
      },
      "source": [
        "# **Tracking with DeepSORT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "CuUBoyXtVxCC"
      },
      "outputs": [],
      "source": [
        "from deep_sort.utils.parser import get_config\n",
        "from deep_sort.deep_sort import DeepSort\n",
        "\n",
        "deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7' # Path to the DeepSort model\n",
        "tracker = DeepSort(model_path=deep_sort_weights, max_age=70) # Initialize the DeepSort tracker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Open the video file for processing\n",
        "cap = cv2.VideoCapture(\"video.mp4\")\n",
        "\n",
        "# Retrieve and store the width of the video frames\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "# Retrieve and store the height of the video frames\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Retrieve and store the frames per second (FPS) of the video\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # check if GPU is available\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "frames = []  # Initialize an empty list to store the frames\n",
        "unique_track_ids = set()  # Initialize an empty set to store the unique track IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "output_path = 'output1.mp4'\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize variables for tracking FPS and elapsed time\n",
        "i = 0\n",
        "counter, fps, elapsed = 0, 0, 0\n",
        "start_time = time()  # Record the start time for FPS calculation\n",
        "\n",
        "# Dictionary to store tracking results, including confidence scores, class names, bounding boxes, and track IDs\n",
        "results_track = {\n",
        "    'conf_scores': [],\n",
        "    'class_names': [],\n",
        "    'bboxes': [],\n",
        "    'track_id': []\n",
        "}\n",
        "\n",
        "# Start reading the video frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()  # Read a frame from the video\n",
        "\n",
        "    if ret:\n",
        "        # Convert the frame from BGR to RGB format (YOLO typically expects RGB)\n",
        "        og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = og_frame.copy()\n",
        "\n",
        "        # Load the YOLOv8 model\n",
        "        model = YOLO(\"yolov8l.pt\")\n",
        "\n",
        "        # Run the model on the current frame with specified settings (device, classes, confidence threshold)\n",
        "        results = model(frame, device=0, classes=0, conf=0.8)\n",
        "\n",
        "        # Process the results from YOLO for each detection\n",
        "        for result in results:\n",
        "            boxes = result.boxes  # Extract bounding boxes\n",
        "            cls = boxes.cls.tolist()  # Extract class indices\n",
        "            xyxy = boxes.xyxy  # Extract bounding box coordinates (x1, y1, x2, y2)\n",
        "            conf = boxes.conf  # Extract confidence scores\n",
        "            xywh = boxes.xywh  # Extract bounding box coordinates (x, y, width, height)\n",
        "            \n",
        "            # Map class indices to class names\n",
        "            for class_index in cls:\n",
        "                class_name = class_names[int(class_index)]\n",
        "\n",
        "        # Convert predictions to numpy arrays and detach from computation graph\n",
        "        pred_cls = np.array(cls)\n",
        "        conf = conf.detach().cpu().numpy()\n",
        "        xyxy = xyxy.detach().cpu().numpy()\n",
        "        bboxes_xywh = xywh.cpu().numpy()\n",
        "\n",
        "        # Store the tracking results in the results_track dictionary\n",
        "        results_track['conf_scores'].extend(conf.tolist())\n",
        "        results_track['class_names'].extend([class_names[int(index)] for index in cls])\n",
        "        results_track['bboxes'].extend(xyxy.tolist())\n",
        "        \n",
        "        # Update the tracker with new bounding boxes and confidence scores\n",
        "        tracks = tracker.update(bboxes_xywh, conf, og_frame)\n",
        "\n",
        "        # Iterate over the tracks to draw bounding boxes and track IDs on the frame\n",
        "        for track in tracker.tracker.tracks:\n",
        "            track_id = track.track_id  # Retrieve the track ID\n",
        "            hits = track.hits  # Number of hits for the track (how often it was detected)\n",
        "            x1, y1, x2, y2 = track.to_tlbr()  # Convert bounding box format to (top-left, bottom-right)\n",
        "            w = x2 - x1  # Calculate the width of the bounding box\n",
        "            h = y2 - y1  # Calculate the height of the bounding box\n",
        "\n",
        "            # Define colors for bounding boxes based on track ID\n",
        "            red_color = (0, 0, 255)  # Red color\n",
        "            blue_color = (255, 0, 0)  # Blue color\n",
        "            green_color = (0, 255, 0)  # Green color\n",
        "\n",
        "            # Cycle through colors based on track ID\n",
        "            color_id = track_id % 3\n",
        "            if color_id == 0:\n",
        "                color = red_color\n",
        "            elif color_id == 1:\n",
        "                color = blue_color\n",
        "            else:\n",
        "                color = green_color\n",
        "\n",
        "            # Draw the bounding box with the appropriate color\n",
        "            cv2.rectangle(og_frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
        "\n",
        "            # Draw the class name and track ID above the bounding box\n",
        "            text_color = (0, 0, 0)  # Black color for text\n",
        "            cv2.putText(og_frame, f\"{class_name}-{track_id}\", (int(x1) + 10, int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1, cv2.LINE_AA)\n",
        "            \n",
        "            # Add the track ID to the set of unique track IDs\n",
        "            unique_track_ids.add(track_id)\n",
        "\n",
        "        # Count the number of unique persons being tracked\n",
        "        person_count = len(unique_track_ids)\n",
        "\n",
        "        # Update the FPS calculation based on elapsed time\n",
        "        current_time = time()\n",
        "        elapsed = (current_time - start_time)\n",
        "        counter += 1\n",
        "        if elapsed > 1:\n",
        "            fps = counter / elapsed\n",
        "            counter = 0\n",
        "            start_time = current_time\n",
        "\n",
        "        # Display the person count on the frame\n",
        "        cv2.putText(og_frame, f\"Person Count: {person_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        # Store the processed frame for output\n",
        "        frames.append(og_frame)\n",
        "\n",
        "        # Write the frame to the output video file\n",
        "        out.write(cv2.cvtColor(og_frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Display the frame with bounding boxes and tracking info\n",
        "        cv2.imshow(\"Video\", og_frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit the loop\n",
        "            break\n",
        "\n",
        "# Release resources after processing is done\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
