{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for object detection using YOLOv8\n",
    "\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import cvzone\n",
    "import math\n",
    "\n",
    "# Define the video path (use a video that has a single person to detect a single person, otherwise it detects multiple people at the same time)\n",
    "cap = cv2.VideoCapture(\"videoplayback.mp4\")\n",
    "\n",
    "model = YOLO(\"../Yolo-Weights/yolov8l.pt\")  # Load the model\n",
    "\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "              \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "              ] # List of class names\n",
    "\n",
    "key = 0  # Initialize key\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()  # Read the video\n",
    "    \n",
    "    if not success:\n",
    "        break  # Exit loop if video has ended\n",
    "\n",
    "    results = model(img, stream=True)  # Detect objects\n",
    "    \n",
    "    detections = np.empty((0, 5))  # Initialize empty array to store detections\n",
    "\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            # Bounding Box\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "\n",
    "            # Confidence\n",
    "            conf = math.ceil((box.conf[0] * 100)) / 100\n",
    "            # Class Name\n",
    "            cls = int(box.cls[0])\n",
    "            currentClass = classNames[cls]\n",
    "\n",
    "            if currentClass == \"person\" and conf > 0.3:\n",
    "                cvzone.putTextRect(img, f'{currentClass} {conf}', (max(0, x1), max(35, y1)),\n",
    "                                scale=0.6, thickness=1, offset=3) # Display the class name and confidence score\n",
    "                cvzone.cornerRect(img, (x1, y1, w, h), l=9, rt=5) # Display the bounding box\n",
    "\n",
    "    cv2.imshow(\"Image\", img)  # Display the image\n",
    "    key = cv2.waitKey(1) & 0xFF  # Wait for key\n",
    "\n",
    "    if key == ord('c') or key == ord('C'):  # Exit if 'c' or 'C' is pressed\n",
    "        break\n",
    "\n",
    "cap.release()  # Release the video capture\n",
    "cv2.destroyAllWindows() # Close all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_sort.utils.parser import get_config\n",
    "from deep_sort.deep_sort import DeepSort\n",
    "\n",
    "deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7' # Path to the DeepSort model\n",
    "tracker = DeepSort(model_path=deep_sort_weights, max_age=70) # Initialize the DeepSort tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "\n",
    "video_path = 'videoplayback.mp4' # Path to the video\n",
    "\n",
    "cap = cv2.VideoCapture(video_path) # Open the video\n",
    "\n",
    "# Get the video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) \n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_path = 'output.mp4'\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "unique_track_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize variables\n",
    "i = 0\n",
    "counter, fps, elapsed = 0, 0, 0\n",
    "start_time = time.perf_counter()\n",
    "person_tracked = False\n",
    "tracked_person_id = None\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")  # Load a pretrained model\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()  # Read the frame\n",
    "\n",
    "    if ret:\n",
    "        og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB - OpenCV uses BGR\n",
    "        frame = og_frame.copy()  # Copy the frame\n",
    "\n",
    "        # Run inference with YOLO model\n",
    "        results = model(frame, device=0, classes=0, conf=0.8)  # Run inference\n",
    "        \n",
    "        class_names = ['person']  # List of class names\n",
    "        bboxes_xywh = []  # List of bounding boxes\n",
    "        confs = []  # List of confidence scores\n",
    "        ids = []  # List of object IDs\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes  # Boxes object for bbox outputs\n",
    "            cls = boxes.cls.tolist()  # Convert tensor to list\n",
    "            xywh = boxes.xywh.cpu().numpy() # x, y, width, height\n",
    "            conf = boxes.conf.cpu().numpy()    # confidence scores\n",
    "\n",
    "            for i, class_index in enumerate(cls): \n",
    "                if class_names[int(class_index)] == 'person':\n",
    "                    bboxes_xywh.append(xywh[i]) \n",
    "                    confs.append(conf[i])\n",
    "                    ids.append(i)\n",
    "\n",
    "        # If a person is already being tracked, continue tracking that person\n",
    "        if person_tracked:\n",
    "            if tracked_person_id in ids:\n",
    "                index = ids.index(tracked_person_id)\n",
    "                tracker_bbox = bboxes_xywh[index]\n",
    "            else:\n",
    "                # If the tracked person is not detected, continue without updating\n",
    "                tracker_bbox = None\n",
    "        else:\n",
    "            # If no person is being tracked, start tracking the first detected person\n",
    "            if len(bboxes_xywh) > 0:\n",
    "                tracker_bbox = bboxes_xywh[0]\n",
    "                tracked_person_id = ids[0]\n",
    "                person_tracked = True\n",
    "            else:\n",
    "                tracker_bbox = None\n",
    "\n",
    "        if tracker_bbox is not None:\n",
    "            x, y, w, h = tracker_bbox # x, y, width, height\n",
    "            cv2.rectangle(og_frame, (int(x-w/2), int(y-h/2)), (int(x+w/2), int(y+h/2)), (0, 255, 0), 2) # Draw the bounding box\n",
    "            cv2.putText(og_frame, f\"Person-{tracked_person_id}\", (int(x) + 10, int(y) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA) # Draw the object ID\n",
    "\n",
    "        # Update FPS and place on frame\n",
    "        current_time = time.perf_counter() # Get the current time\n",
    "        elapsed = (current_time - start_time) # Get the elapsed time\n",
    "        counter += 1 # Increment the frame counter\n",
    "        if elapsed > 1:\n",
    "            fps = counter / elapsed # Calculate the FPS\n",
    "            counter = 0 # Reset the frame counter\n",
    "            start_time = current_time # Reset the start time\n",
    "\n",
    "        # Draw FPS on frame\n",
    "        cv2.putText(og_frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Write the frame to the output video file\n",
    "        out.write(cv2.cvtColor(og_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Video\", og_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release() \n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imageD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
